

==============================================
Directory: terraform/fsx-for-lustre
==============================================
Listing files in the fsx-for-lustre directory:
total 24
drwxr-xr-x   5 antonai  staff  160 Apr 27 16:00 .
drwxr-xr-x  24 antonai  staff  768 Apr 27 17:22 ..
-rw-r--r--   1 antonai  staff  414 Apr 27 16:00 fsxlustre-static-pv.yaml
-rw-r--r--   1 antonai  staff  236 Apr 27 16:00 fsxlustre-static-pvc.yaml
-rw-r--r--   1 antonai  staff  181 Apr 27 16:00 fsxlustre-storage-class.yaml


==============================================
File: terraform/addons.tf
==============================================
#---------------------------------------------------------------
# EKS Blueprints Kubernetes Addons
#---------------------------------------------------------------
module "eks_blueprints_addons" {
  source  = "aws-ia/eks-blueprints-addons/aws"
  version = "~> 1.3"

  cluster_name      = module.eks.cluster_name
  cluster_endpoint  = module.eks.cluster_endpoint
  cluster_version   = module.eks.cluster_version
  oidc_provider_arn = module.eks.oidc_provider_arn

  #---------------------------------------
  # Amazon EKS Managed Add-ons
  #---------------------------------------
  eks_addons = {
    coredns = {
      preserve = true
    }
    vpc-cni = {
      preserve = true
    }
    kube-proxy = {
      preserve = true
    }
    amazon-cloudwatch-observability = {
      preserve                 = true
      service_account_role_arn = aws_iam_role.cloudwatch_observability_role.arn
    }
  }

  #---------------------------------------
  # ALB Controller
  #---------------------------------------
  enable_aws_load_balancer_controller = true

  #---------------------------------------
  # Kubernetes Metrics Server
  #---------------------------------------
  enable_metrics_server = true


  #---------------------------------------
  # Enable FSx for Lustre CSI Driver
  #---------------------------------------
  enable_aws_fsx_csi_driver = true

  tags = local.tags

}

#---------------------------------------------------------------
# Data on EKS Kubernetes Addons
#---------------------------------------------------------------
module "eks_data_addons" {
  source  = "aws-ia/eks-data-addons/aws"
  version = "~> 1.30" # ensure to update this to the latest/desired version

  oidc_provider_arn           = module.eks.oidc_provider_arn
  enable_nvidia_device_plugin = true

}

#---------------------------------------------------------------
# EKS Amazon CloudWatch Observability Role
#---------------------------------------------------------------
resource "aws_iam_role" "cloudwatch_observability_role" {
  name = "eks-cloudwatch-agent-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRoleWithWebIdentity"
        Effect = "Allow"
        Principal = {
          Federated = module.eks.oidc_provider_arn
        }
        Condition = {
          StringEquals = {
            "${replace(module.eks.cluster_oidc_issuer_url, "https://", "")}:sub" : "system:serviceaccount:amazon-cloudwatch:cloudwatch-agent",
            "${replace(module.eks.cluster_oidc_issuer_url, "https://", "")}:aud" : "sts.amazonaws.com"
          }
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "cloudwatch_observability_policy_attachment" {
  policy_arn = "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"
  role       = aws_iam_role.cloudwatch_observability_role.name
}


==============================================
File: terraform/cleanup.sh
==============================================
#!/bin/bash
set -o errexit
set -o pipefail

targets=(
  "module.eks"
  "module.vpc"
)

#-------------------------------------------
# Helpful to delete the stuck in "Terminating" namespaces
# Rerun the cleanup.sh script to detect and delete the stuck resources
#-------------------------------------------
terminating_namespaces=$(kubectl get namespaces --field-selector status.phase=Terminating -o json | jq -r '.items[].metadata.name')

# If there are no terminating namespaces, exit the script
if [[ -z $terminating_namespaces ]]; then
    echo "No terminating namespaces found"
fi

for ns in $terminating_namespaces; do
    echo "Terminating namespace: $ns"
    kubectl get namespace $ns -o json | sed 's/"kubernetes"//' | kubectl replace --raw "/api/v1/namespaces/$ns/finalize" -f -
done

for target in "${targets[@]}"
do
  terraform destroy -target="$target" -auto-approve
  destroy_output=$(terraform destroy -target="$target" -auto-approve 2>&1)
  if [[ $? -eq 0 && $destroy_output == *"Destroy complete!"* ]]; then
    echo "SUCCESS: Terraform destroy of $target completed successfully"
  else
    echo "FAILED: Terraform destroy of $target failed"
    exit 1
  fi
done

terraform destroy -auto-approve
destroy_output=$(terraform destroy -auto-approve 2>&1)
if [[ $? -eq 0 && $destroy_output == *"Destroy complete!"* ]]; then
  echo "SUCCESS: Terraform destroy of all targets completed successfully"
else
  echo "FAILED: Terraform destroy of all targets failed"
  exit 1
fi


==============================================
File: terraform/eks.tf
==============================================
#---------------------------------------------------------------
# EKS Cluster
#---------------------------------------------------------------
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 19.15"

  cluster_name                   = local.name
  cluster_version                = var.eks_cluster_version
  cluster_endpoint_public_access = true # if true, Your cluster API server is accessible from the internet. You can, optionally, limit the CIDR blocks that can access the public endpoint.
  vpc_id                         = module.vpc.vpc_id
  subnet_ids                     = module.vpc.private_subnets
  manage_aws_auth_configmap      = true

  #---------------------------------------
  # Note: This can further restricted to specific required for each Add-on and your application
  #---------------------------------------
  # Extend cluster security group rules
  cluster_security_group_additional_rules = {
    ingress_nodes_ephemeral_ports_tcp = {
      description                = "Nodes on ephemeral ports"
      protocol                   = "tcp"
      from_port                  = 1025
      to_port                    = 65535
      type                       = "ingress"
      source_node_security_group = true
    }
  }

  # Extend node-to-node security group rules
  node_security_group_additional_rules = {
    ingress_self_all = {
      description = "Node to node all ports/protocols"
      protocol    = "-1"
      from_port   = 0
      to_port     = 0
      type        = "ingress"
      self        = true
    }
    # Allows Control Plane Nodes to talk to Worker nodes on all ports. Added this to simplify the example and further avoid issues with Add-ons communication with Control plane.
    # This can be restricted further to specific port based on the requirement for each Add-on e.g., metrics-server 4443, spark-operator 8080, karpenter 8443 etc.
    # Change this according to your security requirements if needed
    ingress_cluster_to_node_all_traffic = {
      description                   = "Cluster API to Nodegroup all traffic"
      protocol                      = "-1"
      from_port                     = 0
      to_port                       = 0
      type                          = "ingress"
      source_cluster_security_group = true
    }
  }

  eks_managed_node_group_defaults = {
    iam_role_additional_policies = {
      # Not required, but used in the example to access the nodes to inspect mounted volumes
      AmazonSSMManagedInstanceCore = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
    }
  }

  eks_managed_node_groups = {
    #  We recommend to have a MNG to place your critical workloads and add-ons
    #  Then rely on Karpenter to scale your workloads
    #  You can also make uses on nodeSelector and Taints/tolerations to spread workloads on MNG or Karpenter provisioners

    core_node_group = {
      name        = "core-node-group"
      description = "EKS Core node group for hosting critical add-ons"
      # Filtering only Secondary CIDR private subnets starting with "100.".
      # Subnet IDs where the nodes/node groups will be provisioned
      subnet_ids = compact([for subnet_id, cidr_block in zipmap(module.vpc.private_subnets, module.vpc.private_subnets_cidr_blocks) :
        substr(cidr_block, 0, 4) == "100." ? subnet_id : null]
      )

      min_size     = 3
      max_size     = 9
      desired_size = 3

      instance_types = ["m5.xlarge"]

      ebs_optimized = true
      block_device_mappings = {
        xvda = {
          device_name = "/dev/xvda"
          ebs = {
            volume_size = 100
            volume_type = "gp3"
          }
        }
      }

      labels = {
        WorkerType    = "ON_DEMAND"
        NodeGroupType = "core"
      }

      tags = merge(local.tags, {
        Name                     = "core-node-grp",
        "karpenter.sh/discovery" = local.name
      })
    }

    gpu1 = {
      name        = "gpu-node-grp"
      description = "Node group originally GPU-based, temporarily using CPU instances for testing"

      subnet_ids = compact([
        for subnet_id, cidr_block in zipmap(module.vpc.private_subnets, module.vpc.private_subnets_cidr_blocks) :
        substr(cidr_block, 0, 4) == "100." ? subnet_id : null
      ])

      # Commented out GPU-specific AMI type (use default AL2 for CPU testing)
      # ami_type     = "AL2_x86_64_GPU"
      min_size     = 1
      max_size     = 4
      desired_size = 1

      # Original GPU instances commented out for testing with CPUs
      # instance_types = ["p3.2xlarge", "p3.8xlarge"]
      instance_types = ["m5.xlarge"]

      labels = {
        WorkerType    = "ON_DEMAND"
        NodeGroupType = "gpu"
      }

      # Taints originally for GPU-specific workloads are temporarily disabled
      # taints = [{
      #   key    = "nvidia.com/gpu"
      #   value  = "true"
      #   effect = "NO_SCHEDULE"
      # }]

      tags = merge(local.tags, {
        Name = "gpu-node-grp"
      })
    }

  }
}


==============================================
File: terraform/fsx-for-lustre.tf
==============================================
#---------------------------------------------------------------
# FSx for Lustre File system Static provisioning
#    1> Create Fsx for Lustre filesystem (Lustre FS storage capacity must be 1200, 2400, or a multiple of 3600)
#    2> Create Storage Class for Filesystem (Cluster scoped)
#    3> Persistent Volume with  Hardcoded reference to Fsx for Lustre filesystem with filesystem_id and dns_name (Cluster scoped)
#    4> Persistent Volume claim for this persistent volume will always use the same file system (Namespace scoped)
#---------------------------------------------------------------
# NOTE: FSx for Lustre file system creation can take up to 10 mins
resource "aws_fsx_lustre_file_system" "this" {
  deployment_type             = "PERSISTENT_2"
  storage_type                = "SSD"
  per_unit_storage_throughput = "500" # 125, 250, 500, 1000
  storage_capacity            = 1200

  subnet_ids         = [module.vpc.private_subnets[0]]
  security_group_ids = [aws_security_group.fsx.id]
  log_configuration {
    level = "WARN_ERROR"
  }
  tags = merge({ "Name" : "${local.name}-static" }, local.tags)
}

# This process can take upto 7 mins
resource "aws_fsx_data_repository_association" "this" {

  file_system_id       = aws_fsx_lustre_file_system.this.id
  data_repository_path = "s3://${module.fsx_s3_bucket.s3_bucket_id}"
  file_system_path     = "/data" # This directory will be used in Spark podTemplates under volumeMounts as subPath

  s3 {
    auto_export_policy {
      events = ["NEW", "CHANGED", "DELETED"]
    }

    auto_import_policy {
      events = ["NEW", "CHANGED", "DELETED"]
    }
  }
}

#---------------------------------------------------------------
# Sec group for FSx for Lustre
#---------------------------------------------------------------
resource "aws_security_group" "fsx" {

  name        = "${local.name}-fsx"
  description = "Allow inbound traffic from private subnets of the VPC to FSx filesystem"
  vpc_id      = module.vpc.vpc_id

  ingress {
    description = "Allows Lustre traffic between Lustre clients"
    cidr_blocks = module.vpc.private_subnets_cidr_blocks
    from_port   = 1021
    to_port     = 1023
    protocol    = "tcp"
  }
  ingress {
    description = "Allows Lustre traffic between Lustre clients"
    cidr_blocks = module.vpc.private_subnets_cidr_blocks
    from_port   = 988
    to_port     = 988
    protocol    = "tcp"
  }
  tags = local.tags
}

#---------------------------------------------------------------
# S3 bucket for DataSync between FSx for Lustre and S3 Bucket
#---------------------------------------------------------------
#tfsec:ignore:aws-s3-enable-bucket-logging tfsec:ignore:aws-s3-enable-versioning
module "fsx_s3_bucket" {
  source  = "terraform-aws-modules/s3-bucket/aws"
  version = "~> 3.0"

  create_bucket = true

  bucket_prefix = "${local.name}-fsx-"
  # For example only - please evaluate for your environment
  force_destroy = true

  server_side_encryption_configuration = {
    rule = {
      apply_server_side_encryption_by_default = {
        sse_algorithm = "AES256"
      }
    }
  }
}

#---------------------------------------------------------------
# Storage Class - FSx for Lustre
#---------------------------------------------------------------
resource "kubectl_manifest" "storage_class" {

  yaml_body = templatefile("${path.module}/fsx-for-lustre/fsxlustre-storage-class.yaml", {
    subnet_id         = module.vpc.private_subnets[0],
    security_group_id = aws_security_group.fsx.id
  })

  depends_on = [
    module.eks_blueprints_addons
  ]
}

#---------------------------------------------------------------
# FSx for Lustre Persistent Volume - Static provisioning
#---------------------------------------------------------------
resource "kubectl_manifest" "static_pv" {

  yaml_body = templatefile("${path.module}/fsx-for-lustre/fsxlustre-static-pv.yaml", {
    filesystem_id = aws_fsx_lustre_file_system.this.id,
    dns_name      = aws_fsx_lustre_file_system.this.dns_name
    mount_name    = aws_fsx_lustre_file_system.this.mount_name,
  })

  depends_on = [
    module.eks_blueprints_addons,
    kubectl_manifest.storage_class,
    aws_fsx_lustre_file_system.this
  ]
}

#---------------------------------------------------------------
# FSx for Lustre Persistent Volume Claim
#---------------------------------------------------------------
resource "kubectl_manifest" "static_pvc" {

  yaml_body = templatefile("${path.module}/fsx-for-lustre/fsxlustre-static-pvc.yaml", {})

  depends_on = [
    module.eks_blueprints_addons,
    kubectl_manifest.storage_class,
    kubectl_manifest.static_pv,
    aws_fsx_lustre_file_system.this
  ]
}


==============================================
File: terraform/iam_policies.tf
==============================================
resource "aws_iam_policy" "terraform_consolidated_policy" {
  name        = "terraform_consolidated_admin_policy"
  path        = "/"
  description = "Consolidated Terraform admin permissions for KMS, S3, RDS, and EC2."

  policy = file("${path.module}/terraform-consolidated-policy.json")
}

resource "aws_iam_user_policy_attachment" "terraform_consolidated_attachment" {
  user       = "container"
  policy_arn = aws_iam_policy.terraform_consolidated_policy.arn
}


==============================================
File: terraform/install.sh
==============================================
#!/bin/bash

# List of Terraform modules to apply in sequence
targets=(
  "module.vpc"
  "module.eks"
)

# Initialize Terraform
echo "Initializing ..."
terraform init --upgrade || echo "\"terraform init\" failed"

# Initialize Terraform
echo "Initializing ..."
terraform init --upgrade || { echo "\"terraform init\" failed"; exit 1; }

# Apply IAM policies first to resolve permission errors
echo "Applying IAM policies..."
terraform apply -target=aws_iam_policy -auto-approve || { echo "FAILED: IAM policies apply failed"; exit 1; }



echo "SUCCESS: IAM policies applied successfully"


# Apply modules in sequence
for target in "${targets[@]}"
do
  echo "Applying module $target..."
  apply_output=$(terraform apply -target="$target" -auto-approve 2>&1 | tee /dev/tty)
  if [[ ${PIPESTATUS[0]} -eq 0 && $apply_output == *"Apply complete"* ]]; then
    echo "SUCCESS: Terraform apply of $target completed successfully"
  else
    echo "FAILED: Terraform apply of $target failed"
    exit 1
  fi
done

# Final apply to catch any remaining resources
echo "Applying remaining resources..."
apply_output=$(terraform apply -auto-approve 2>&1 | tee /dev/tty)
if [[ ${PIPESTATUS[0]} -eq 0 && $apply_output == *"Apply complete"* ]]; then
  echo "SUCCESS: Terraform apply of all modules completed successfully"
else
  echo "FAILED: Terraform apply of all modules failed"
  exit 1
fi


==============================================
File: terraform/main.tf
==============================================
provider "aws" {
  region = local.region
}

provider "kubernetes" {
  host                   = module.eks.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
  token                  = data.aws_eks_cluster_auth.this.token
}

# ECR always authenticates with `us-east-1` region
# Docs -> https://docs.aws.amazon.com/AmazonECR/latest/public/public-registries.html
provider "aws" {
  alias  = "ecr"
  region = "us-east-1"
}

provider "helm" {
  kubernetes {
    host                   = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
    token                  = data.aws_eks_cluster_auth.this.token
  }
}

provider "kubectl" {
  apply_retry_count      = 10
  host                   = module.eks.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
  load_config_file       = false
  token                  = data.aws_eks_cluster_auth.this.token
}

data "aws_availability_zones" "available" {}

data "aws_eks_cluster_auth" "this" {
  name = module.eks.cluster_name
}

#---------------------------------------------------------------
# Local variables
#---------------------------------------------------------------
locals {
  name     = var.name
  region   = var.region
  vpc_cidr = var.vpc_cidr
  azs      = slice(data.aws_availability_zones.available.names, 0, 2)

  tags = {
    Blueprint  = local.name
    GithubRepo = "github.com/awslabs/data-on-eks"
  }
}


==============================================
File: terraform/outputs.tf
==============================================
output "configure_kubectl" {
  description = "Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig"
  value       = "aws eks --region ${local.region} update-kubeconfig --alias ${module.eks.cluster_name} --name ${module.eks.cluster_name}"
}

output "eks_api_server_url" {
  description = "Your eks API server endpoint"
  value       = module.eks.cluster_endpoint
}


==============================================
File: terraform/slinky_pre_req.tf
==============================================
File not found: /Users/antonai/github_aws/terraform/slinky_pre_req.tf


==============================================
File: terraform/slinky.tf
==============================================
File not found: /Users/antonai/github_aws/terraform/slinky.tf


==============================================
File: terraform/variables.tf
==============================================
variable "name" {
  description = "Name of the VPC and hyperpod Cluster"
  default     = "slinky-on-eks"
  type        = string
}

variable "region" {
  description = "Region"
  type        = string
  default     = "us-east-1"
}

variable "eks_cluster_version" {
  description = "EKS Cluster version"
  default     = "1.29"
  type        = string
}

# VPC with 2046 IPs (10.1.0.0/21) and 2 AZs
variable "vpc_cidr" {
  description = "VPC CIDR"
  default     = "10.1.0.0/21"
  type        = string
}

# RFC6598 range 100.64.0.0/10
# Note you can only /16 range to VPC. You can add multiples of /16 if required
variable "secondary_cidr_blocks" {
  description = "Secondary CIDR blocks to be attached to VPC"
  default     = ["100.64.0.0/16"]
  type        = list(string)
}


==============================================
File: terraform/versions.tf
==============================================
terraform {
  required_version = ">= 1.0.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 3.72"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.20.0"
    }
    helm = {
      source  = "hashicorp/helm"
      version = ">= 2.9.0"
    }
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = ">= 1.14"
    }
  }

  # ##  Used for end-to-end testing on project; update to suit your needs
  # backend "s3" {
  #   bucket = "doeks-github-actions-e2e-test-state"
  #   region = "us-east-1"
  #   key    = "e2e/bionemo/terraform.tfstate"
  # }
}


==============================================
File: terraform/vpc.tf
==============================================
# vpc.tf

#---------------------------------------------------------------
# VPC Configuration
# This is a dev/demo setup that demonstrates key patterns.
# For production, consider: multiple NAT gateways, VPC endpoints, and flow logs.
#---------------------------------------------------------------

locals {
  name   = var.cluster_name
  region = var.region
  azs    = slice(data.aws_availability_zones.available.names, 0, var.az_count) # Default to 2 AZs for dev

  # CIDR calculations
  # Primary VPC CIDR - standard RFC1918 private range
  vpc_cidr = var.vpc_cidr  # Default: "10.0.0.0/16"
  
  # Private subnets for EKS nodes and pods (in primary CIDR)
  private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 4, k)]
  
  # Public subnets for load balancers and bastion hosts (in primary CIDR)
  public_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 16)]
  
  # Database subnets (in primary CIDR)
  database_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 24)]
  
  # Secondary CIDR for pod IPs if needed (uses RFC6598 range)
  # This demonstrates how to set up a secondary CIDR if more IP addresses are needed
  use_secondary_cidr = var.enable_secondary_cidr
  secondary_cidr = var.secondary_cidr  # Default: "100.64.0.0/16"
  secondary_private_subnets = local.use_secondary_cidr ? 
    [for k, v in local.azs : cidrsubnet(local.secondary_cidr, 2, k)] : []
}

module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  name = "${local.name}-vpc"
  cidr = local.vpc_cidr
  azs  = local.azs

  # Use secondary CIDR only if enabled - useful for large clusters with many pods
  secondary_cidr_blocks = local.use_secondary_cidr ? [local.secondary_cidr] : []

  # Subnets - dev setup with reasonable sizes
  private_subnets = local.use_secondary_cidr ? 
    concat(local.private_subnets, local.secondary_private_subnets) : local.private_subnets
  public_subnets  = local.public_subnets
  database_subnets = local.database_subnets

  # For dev environment: single NAT gateway to reduce costs
  # In production, use one NAT gateway per AZ for high availability
  enable_nat_gateway = true
  single_nat_gateway = true
  
  # DNS settings - required for EKS
  enable_dns_hostnames = true
  enable_dns_support = true

  # Create a database subnet group for RDS if needed
  create_database_subnet_group = true
  create_database_subnet_route_table = true

  # IMPORTANT: Tags for Kubernetes to discover subnets
  # These allow the AWS Load Balancer Controller to find the right subnets
  public_subnet_tags = {
    "kubernetes.io/role/elb" = 1
    "kubernetes.io/cluster/${local.name}" = "shared"
  }

  private_subnet_tags = {
    "kubernetes.io/role/internal-elb" = 1
    "kubernetes.io/cluster/${local.name}" = "shared"
  }
  
  tags = var.tags
}

# Optional: Security group for FSx Lustre
# Uncomment when you need FSx for Lustre integration
resource "aws_security_group" "fsx" {
  count = var.create_fsx_security_group ? 1 : 0
  
  name        = "${local.name}-fsx-security-group"
  description = "Security group for FSx Lustre filesystem"
  vpc_id      = module.vpc.vpc_id

  ingress {
    description = "Allow Lustre traffic"
    from_port   = 988
    to_port     = 988
    protocol    = "tcp"
    cidr_blocks = module.vpc.private_subnets_cidr_blocks
  }

  ingress {
    description = "Allow Lustre management traffic"
    from_port   = 1021
    to_port     = 1023
    protocol    = "tcp"
    cidr_blocks = module.vpc.private_subnets_cidr_blocks
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = var.tags
}
apiVersion: slinky.slurm.net/v1alpha1
kind: NodeSet
metadata:
  annotations:
    meta.helm.sh/release-name: slurm
    meta.helm.sh/release-namespace: slurm
  creationTimestamp: "2025-03-22T05:53:08Z"
  generation: 1
  labels:
    app.kubernetes.io/component: compute
    app.kubernetes.io/instance: slurm-compute-debug
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: slurmd
    app.kubernetes.io/version: "24.05"
    helm.sh/chart: slurm-0.1.0
  name: slurm-compute-debug
  namespace: slurm
  resourceVersion: "14461"
  uid: 54b8028c-46ce-4764-aecc-be2a190276b4
spec:
  clusterName: slurm
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
  replicas: 1
  revisionHistoryLimit: 0
  selector:
    matchLabels:
      app.kubernetes.io/instance: slurm-compute-debug
      app.kubernetes.io/name: slurmd
  serviceName: slurm-compute
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: slurmd
      labels:
        app.kubernetes.io/component: compute
        app.kubernetes.io/instance: slurm-compute-debug
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: slurmd
        app.kubernetes.io/version: "24.05"
        helm.sh/chart: slurm-0.1.0
    spec:
      automountServiceAccountToken: false
      containers:
      - args:
        - -D
        - -Z
        - --conf-server
        - slurm-controller:6817
        - --conf
        - Features=debug Gres=gpu:4 Weight=1
        image: ghcr.io/slinkyproject/slurmd:24.05-ubuntu-24.04
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/bash
              - -c
              - scontrol update nodename=$(hostname) state=down reason=preStop &&
                scontrol delete nodename=$(hostname);
        name: slurmd
        ports:
        - containerPort: 6818
          name: slurmd
          protocol: TCP
        readinessProbe:
          exec:
            command:
            - scontrol
            - show
            - slurmd
        resources:
          limits:
            nvidia.com/gpu: "4"
          requests:
            nvidia.com/gpu: "4"
        securityContext:
          capabilities:
            add:
            - BPF
            - NET_ADMIN
            - SYS_ADMIN
            - SYS_NICE
          privileged: true
        startupProbe:
          exec:
            command:
            - scontrol
            - show
            - slurmd
        volumeMounts:
        - mountPath: /etc/slurm
          name: etc-slurm
        - mountPath: /var/run
          name: run
      dnsConfig:
        searches:
        - slurm-controller.slurm.svc.cluster.local
        - slurm-compute.slurm.svc.cluster.local
      hostname: slurm-compute-debug
      initContainers:
      - command:
        - bash
        - -c
        - "#!/usr/bin/env bash\n# SPDX-FileCopyrightText: Copyright (C) SchedMD LLC.\n#
          SPDX-License-Identifier: Apache-2.0\n\nset -euo pipefail\n\n# Assume env
          contains:\n# SLURM_USER - username or UID\n\nfunction init::common() {\n\tlocal
          dir\n\n\tdir=/var/spool/slurmd\n\tmkdir -p \"$dir\"\n\tchown -v \"${SLURM_USER}:${SLURM_USER}\"
          \"$dir\"\n\tchmod -v 700 \"$dir\"\n\n\tdir=/var/spool/slurmctld\n\tmkdir
          -p \"$dir\"\n\tchown -v \"${SLURM_USER}:${SLURM_USER}\" \"$dir\"\n\tchmod
          -v 700 \"$dir\"\n}\n\nfunction init::slurm() {\n\tSLURM_MOUNT=/mnt/slurm\n\tSLURM_DIR=/mnt/etc/slurm\n\n\t#
          Workaround to ephemeral volumes not supporting securityContext\n\t# https://github.com/kubernetes/kubernetes/issues/81089\n\n\t#
          Copy Slurm config files, secrets, and scripts\n\tmkdir -p \"$SLURM_DIR\"\n\tfind
          \"${SLURM_MOUNT}\" -type f -name \"*.conf\" -print0 | xargs -0r cp -vt \"${SLURM_DIR}\"\n\tfind
          \"${SLURM_MOUNT}\" -type f -name \"*.key\" -print0 | xargs -0r cp -vt \"${SLURM_DIR}\"\n\n\t#
          Set general permissions and ownership\n\tfind \"${SLURM_DIR}\" -type f -print0
          | xargs -0r chown -v \"${SLURM_USER}:${SLURM_USER}\"\n\tfind \"${SLURM_DIR}\"
          -type f -name \"*.conf\" -print0 | xargs -0r chmod -v 644\n\tfind \"${SLURM_DIR}\"
          -type f -name \"*.key\" -print0 | xargs -0r chmod -v 600\n\n\t# Inject secrets
          into certain config files\n\tlocal dbd_conf=\"slurmdbd.conf\"\n\tif [[ -f
          \"${SLURM_MOUNT}/${dbd_conf}\" ]]; then\n\t\techo \"Injecting secrets from
          environment into: ${dbd_conf}\"\n\t\trm -f \"${SLURM_DIR}/${dbd_conf}\"\n\t\tenvsubst
          <\"${SLURM_MOUNT}/${dbd_conf}\" >\"${SLURM_DIR}/${dbd_conf}\"\n\t\tchown
          -v \"${SLURM_USER}:${SLURM_USER}\" \"${SLURM_DIR}/${dbd_conf}\"\n\t\tchmod
          -v 600 \"${SLURM_DIR}/${dbd_conf}\"\n\tfi\n\n\t# Display Slurm directory
          files\n\tls -lAF \"${SLURM_DIR}\"\n}\n\nfunction main() {\n\tinit::common\n\tinit::slurm\n}\nmain\n"
        env:
        - name: SLURM_USER
          value: slurm
        image: ghcr.io/slinkyproject/slurmd:24.05-ubuntu-24.04
        imagePullPolicy: IfNotPresent
        name: init
        resources: {}
        volumeMounts:
        - mountPath: /mnt/slurm
          name: slurm-config
        - mountPath: /mnt/etc/slurm
          name: etc-slurm
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
      - emptyDir:
          medium: Memory
        name: etc-slurm
      - emptyDir: {}
        name: run
      - emptyDir: {}
        name: authsocket
      - name: slurm-config
        projected:
          defaultMode: 384
          sources:
          - secret:
              name: slurm-auth-key
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 20%
      partition: 0
      paused: false
    type: RollingUpdate
status:
  collisionCount: 0
  currentNumberScheduled: 0
  desiredNumberScheduled: 1
  nodeSetHash: 5b4b8948ff
  numberMisscheduled: 0
  numberReady: 0
  numberUnavailable: 1
  observedGeneration: 1
  selector: app.kubernetes.io/instance=slurm-compute-debug,app.kubernetes.io/name=slurmd
